{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LGuN1b2b5PW"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Maybe don't need\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "COdmTGquLKgV"
      },
      "outputs": [],
      "source": [
        "# Other imports\n",
        "!pip install meteostat\n",
        "from datetime import datetime\n",
        "from meteostat import Hourly\n",
        "from meteostat import Point\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M68AQBAh1OUm"
      },
      "outputs": [],
      "source": [
        "# Testing Variables\n",
        "numStations = 1    # Change num to len(fips) for full test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBpnsdguIbs6"
      },
      "outputs": [],
      "source": [
        "# Set time period for data imports\n",
        "startTime = datetime(2018, 1, 1)\n",
        "endTime = datetime(2018, 12, 31, 23, 59)\n",
        "\n",
        "startTime = pd.to_datetime(startTime)\n",
        "endTime = pd.to_datetime(endTime)\n",
        "timeSeries = pd.date_range(start=startTime, end=endTime, freq='15min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kpcNq9Lhf1Qh"
      },
      "outputs": [],
      "source": [
        "# Pull data for centroid of US counties\n",
        "tempData = pd.read_excel(\"us-county-boundaries.xlsx\")\n",
        "countyData = tempData[['GEOID', 'Geo Point']]\n",
        "countyData.sort_values(by = ['GEOID'], inplace=True)\n",
        "countyData[['lat', 'long']] = countyData['Geo Point'].str.split(',', expand=True)\n",
        "countyData.drop(columns=['Geo Point'], inplace=True)\n",
        "print(countyData.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ca2nce6jimGF"
      },
      "outputs": [],
      "source": [
        "# Fetch weather data in US in time range\n",
        "weatherData = pd.DataFrame()\n",
        "for i in range(numStations):\n",
        "  row = countyData.iloc[i]\n",
        "  latTemp = float(row['lat'])\n",
        "  longTemp = float(row['long'])\n",
        "  tempPoint = Point(latTemp, longTemp)\n",
        "  temp = Hourly(tempPoint, startTime, endTime)\n",
        "  dataTemp = temp.fetch()\n",
        "  if(not dataTemp.empty):\n",
        "    dataTemp.drop('dwpt', axis=1, inplace=True)\n",
        "    dataTemp.drop('snow', axis=1, inplace=True)\n",
        "    dataTemp.drop('wpgt', axis=1, inplace=True)\n",
        "    dataTemp.drop('tsun', axis=1, inplace=True)\n",
        "    dataTemp.drop('coco', axis=1, inplace=True)\n",
        "\n",
        "    # Rename Data to unique identifiers\n",
        "    dataTemp.rename(columns={'temp': str(row['GEOID']) + 'temp'}, inplace=True)\n",
        "    dataTemp.rename(columns={'rhum': str(row['GEOID']) + 'rhum'}, inplace=True)\n",
        "    dataTemp.rename(columns={'prcp': str(row['GEOID']) + 'prcp'}, inplace=True)\n",
        "    dataTemp.rename(columns={'wdir': str(row['GEOID']) + 'wdir'}, inplace=True)\n",
        "    dataTemp.rename(columns={'wspd': str(row['GEOID']) + 'wspd'}, inplace=True)\n",
        "    dataTemp.rename(columns={'pres': str(row['GEOID']) + 'pres'}, inplace=True)\n",
        "\n",
        "    # Begin Joining Data w/ unique column names\n",
        "    if weatherData.empty:\n",
        "      weatherData = dataTemp\n",
        "      print('reset the Dataframe')\n",
        "    else:\n",
        "      weatherData = pd.concat([weatherData, dataTemp], axis=1)\n",
        "      print('concat with dataframe')\n",
        "  else:\n",
        "    print('Empty dataTemp')\n",
        "  # Add delay in fetching data to prevent query errors. Experimentation required for reasonable amount of sleep\n",
        "  time.sleep(.25)\n",
        "\n",
        "# Store this data so it doesn't need to be loaded everytime\n",
        "\n",
        "# Print DataFrame\n",
        "print(weatherData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jMtmIQDfNRlQ"
      },
      "outputs": [],
      "source": [
        "# Clean Data\n",
        "weatherData15min = weatherData.resample('15min').mean()\n",
        "weatherData15min = weatherData15min.interpolate()\n",
        "weatherData15min = weatherData15min.bfill()\n",
        "weatherData15min = weatherData15min.dropna(axis=1)\n",
        "\n",
        "# Ensure index matches neccesary format. Including last 45 minutes\n",
        "    # Extra forward and backfill ensure that the data includes first and last hour if not included\n",
        "    # Need to add a method to ensure we are not just forward and backfilling most of a column\n",
        "weatherData15min = weatherData15min.reindex(timeSeries)\n",
        "weatherData15min = weatherData15min.ffill()\n",
        "weatherData15min = weatherData15min.bfill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl_TeJcaHDrY"
      },
      "outputs": [],
      "source": [
        "# Standardize the Weather Data set\n",
        "scaler = StandardScaler()\n",
        "weatherData15min = scaler.fit_transform(weatherData15min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-kRqJrOWFULR"
      },
      "outputs": [],
      "source": [
        "# Pull Outage Data into Program\n",
        "tempData = pd.read_csv(\"eaglei_outages_2018.csv\")\n",
        "outageData = tempData[['run_start_time', 'fips_code']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6pCEgHI1jLnk"
      },
      "outputs": [],
      "source": [
        "# Fetching Outage Data\n",
        "for i in range(numStations):\n",
        "  # Get Data\n",
        "  curFips = countyData.iloc[i]['GEOID']\n",
        "  temp = outageData.loc[outageData['fips_code'].isin([curFips])]\n",
        "\n",
        "  # Adjust Data to have run_start_time as index, and values as 1 or 0 based on outage.\n",
        "  temp['run_start_time'] = pd.to_datetime(temp['run_start_time'], format='%Y-%m-%d %H:%M:%S')\n",
        "  temp.set_index('run_start_time', inplace=True)\n",
        "  temp.loc[temp['fips_code'] == curFips, 'fips_code'] = 1\n",
        "  temp.rename(columns={'fips_code': str(curFips)}, inplace=True)\n",
        "\n",
        "  # Concatenate data onto Outage Data 2 set\n",
        "  if i == 0:\n",
        "    outageData2 = pd.DataFrame(temp, index = timeSeries)\n",
        "  else:\n",
        "    outageData2 = outageData2.join(temp)\n",
        "\n",
        "# Fill NaN with 0, to be readable by ML model\n",
        "outageData2 = outageData2.fillna(0)\n",
        "\n",
        "# Explore the Output\n",
        "print(outageData2.head())\n",
        "print(outageData2.shape)\n",
        "outageData2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ8f8DVtef3_"
      },
      "outputs": [],
      "source": [
        "# Create training and testing sets\n",
        "x = weatherData15min\n",
        "y = outageData2\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
        "inputShape = x_train.shape[1]\n",
        "print(inputShape)\n",
        "outputShape = y_train.shape[1]\n",
        "print(outputShape)\n",
        "\n",
        "# Reshape the input data to have correct shape for CNN\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
        "print(x_train.shape)\n",
        "\n",
        "# Compute Class Weights\n",
        "class_weights = {}\n",
        "count = {0: 0, 1: 0}\n",
        "for col in y_train.columns:\n",
        "  tempCount = y_train[col].value_counts()\n",
        "  count[0] += tempCount.get(0,0)\n",
        "  count[1] += tempCount.get(1,0)\n",
        "print(count)\n",
        "if(count[0] > count[1]):\n",
        "  class_weights[0] = count[0] / count[0]\n",
        "  class_weights[1] = count[0] / count[1]\n",
        "else:\n",
        "  class_weights[0] = count[1] / count[0]\n",
        "  class_weights[1] = count[1] / count[1]\n",
        "print(class_weights)\n",
        "\n",
        "# Convert y data into array\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-26sc4dXI0I9"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, min_lr=1e-6)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = '/tmp/ckpt/checkpoint.model.keras', save_best_only=True, monitor='test_loss', mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X72Q-Sq7SEG"
      },
      "outputs": [],
      "source": [
        "def createModel(cnnLayersParam, denseLayersParam, cnnFilterParam, denseUnitsParam, dropoutParam):\n",
        "  modelTemp = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add CNN Layers\n",
        "  for i in range(cnnLayersParam):\n",
        "    modelTemp.add(tf.keras.layers.Conv1D(filters=cnnFilterParam*(i+1), kernel_size=3, padding='same', activation='relu', input_shape=(inputShape,1)))\n",
        "    modelTemp.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  # Add Flatten Layer\n",
        "  modelTemp.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  # Add Dense Layers\n",
        "  for i in range(denseLayersParam-1):\n",
        "    modelTemp.add(tf.keras.layers.Dense(units=denseUnitsParam*(i+1), activation='relu'))\n",
        "    modelTemp.add(tf.keras.layers.Dropout(dropoutParam))\n",
        "\n",
        "  # Add Output Layer\n",
        "  modelTemp.add(tf.keras.layers.Dense(units=outputShape, activation='sigmoid'))\n",
        "\n",
        "  return modelTemp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep-gFWkR6T7y"
      },
      "outputs": [],
      "source": [
        "# Define model params: Big test\n",
        "#cnnLayers = [2,3,4,5,6]\n",
        "#denseLayers = [2,3,4,5,6]\n",
        "#cnnFilter = [16,32,64]\n",
        "#denseUnits = [32,64,128]\n",
        "#dropout = [0.2,0.3,0.4,0.5]\n",
        "\n",
        "# Define model params: Small Test\n",
        "cnnLayers = [3,4]\n",
        "denseLayers = [3,4]\n",
        "cnnFilter = [16,32]\n",
        "denseUnits = [32]\n",
        "dropout = [0.2,0.4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oNbM2PM68YDD"
      },
      "outputs": [],
      "source": [
        "scores = {}\n",
        "count = 0\n",
        "for i in cnnLayers:\n",
        "  for j in denseLayers:\n",
        "    for k in cnnFilter:\n",
        "      for l in denseUnits:\n",
        "        for m in dropout:\n",
        "          model = createModel(i, j, k, l, m)\n",
        "          model.compile(optimizer='adam',\n",
        "                        loss='binary_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "          history = model.fit(x_train, y_train,\n",
        "                              epochs=100,\n",
        "                              batch_size=128,\n",
        "                              validation_split = 0.2,\n",
        "                              callbacks=[early_stopping, reduce_lr],\n",
        "                              class_weight = class_weights,\n",
        "                              shuffle=False,\n",
        "                              verbose=0)\n",
        "          scores[count] = [i,j,k,l,m, history.history['val_accuracy'][-1], history.history['val_loss'][-1], history.history['accuracy'][-1], history.history['loss'][-1]]\n",
        "          count += 1\n",
        "          print(f\"Validation accuracy: {history.history['val_accuracy'][-1]}    \\tValidation loss: {history.history['val_loss'][-1]}\")\n",
        "\n",
        "print(f\"The parameters with the best validation accuracy is: {max(scores, key = lambda i : i[5])[:]}\")\n",
        "print(f\"The parameters with the best validation loss is: {min(scores, key = lambda i : i[6])[:]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JSowBluy9xTb"
      },
      "outputs": [],
      "source": [
        "print(f\"The parameters with the best validation accuracy is: {max(scores, key = lambda i : i[5])[:]}\")\n",
        "print(f\"The parameters with the best validation loss is: {min(scores, key = lambda i : i[6])[:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jZx3HxhjIh0M"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(scores, columns=['cnnLayers', 'denseLayers', 'cnnFilter', 'denseUnits', 'dropout', 'val_accuracy', 'val_loss'])\n",
        "df.to_csv('scores.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}